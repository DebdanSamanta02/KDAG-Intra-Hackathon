# KDAG-Intra-Hackathon
KDAG Intras Hackathon- Classifying Benign and Malignant Skin Lesions based on Metadata and Images

## Exploratory Data Analysis and Feature Engineering
- The Dataset is HIGHLY Imbalanced with 0s making up 99.9% of the data. Thus Oversampling and Undersampling techniques are necessary for evaluation
- Plotting the distribution of features with 'target' label does not give any actionable insights
- Thus the Feature Engineering depends highly on Real-Life Indicators of Malignant Skin Lesions. The Engineered features were generated by ChatGPT as real medical indicators of malignant skin lesions. These features and their credibility were finally verified manually
- Plotting the distribution of the Engineered Features also does not reveal any significant indicatiors. However the correlation plots of the 'target' label v/s the features show that there is significant correlation among the Engineered Features and the Target (more than the original features)
- Finally, since such tasks generally call for Anomaly Detection, on applying PCA to retain 95% variance of 'target' and plotting the 2-axis PCA plot, there is no obvious way to predict anomailes. K-Means with K=2 works poorly, thus we resort to Tabular Inference Models

## The Tabular Data Models
- The Pipeline provided does the relevant feature engineering and feature selection as learnt from the EDA and Feature Engineering Sections. Further, it scales all numerical columns using ScikitLearn StandardScaler and encodes all categorical columns Ordinally
- The Metrics used are a custom partial ROC-AUC metric (pAUC)- this evaluates the ROC-AUC metric only in the sections where False Negatives are low (malignant cases assigned to be benign)
- We also use the Recall Score and Confusion Matrix to evaluate our models
- Training and validation is done by Undersampling the initial data and using SMOTE to bring the distributions of 1s and 0s closer. This is done over K-Folds (K=5) to generalise model performance
- Cross Validation is done in a similar way but without using SMOTE. This is the metric that we report for the models
- Model Hyperparamaters were tuned using Optuna and the two final models considered are an Ensemble of XGBoost and LightGBM, and a PyTorch-TabNet architecture. While it was difficult to surely predict that one model works better than the other, the averaged K-Folds Inference seems to perform better for the Ensemble discussed. Thus we consider that our final Tabular Model

## The Image Data Model
- Added MobileNetV2 backbone with GAP+GMP pooling and custom dense head, trained with binary focal loss
- Applied aggressive augmentations (crop/flip/rotate/zoom/brightness-contrast) exclusively to malignant images to improve minority diversity
- Implemented stratified positive/negative splits and a balanced batch generator (60% pos / 40% neg) to stabilise training
- Standardised preprocessing: augmentations during training only; deterministic resize + mobilenet normalization for inference
- Enabled saving/loading of best weights and per-isic_id probability outputs, ensuring alignment with tabular predictions for ensemble use
- Updated evaluation focus to recall, PR-AUC, F1, and partial AUC in low false-negative regions; de-emphasised accuracy due to extreme imbalance

## The Integrated Model
- Using the ensemble defined in the Tabular Model section and the MobileNetV2 trained in the Image Data Model section, we predict probabilities over K-Folds models averaged.
- Next we get the simple average of scores from both the Ensemble and the MobileNetV2 and consider these as our final probabilities.
- Threshold tuning did not provide consistent results so we keep the threshold at the default value
- This gives us the final predictions
