# KDAG-Intra-Hackathon
KDAG Intras Hackathon- Classifying Benign and Malignant Skin Lesions based on Metadata and Images

## Exploratory Data Analysis and Feature Engineering
- The Dataset is HIGHLY Imbalanced with 0s making up 99.9% of the data. Thus Oversampling and Undersampling techniques are necessary for evaluation
- Plotting the distribution of features with 'target' label does not give any actionable insights
- Thus the Feature Engineering depends highly on Real-Life Indicators of Malignant Skin Lesions. The Engineered features were generated by ChatGPT as real medical indicators of malignant skin lesions. These features and their credibility were finally verified manually
- Plotting the distribution of the Engineered Features also does not reveal any significant indicatiors. However the correlation plots of the 'target' label v/s the features show that there is significant correlation among the Engineered Features and the Target (more than the original features)
- Finally, since such tasks generally call for Anomaly Detection, on applying PCA to retain 95% variance of 'target' and plotting the 2-axis PCA plot, there is no obvious way to predict anomailes. K-Means with K=2 works poorly, thus we resort to Tabular Inference Models

## The Tabular Data Models
- The Pipeline provided does the relevant feature engineering and feature selection as learnt from the EDA and Feature Engineering Sections. Further, it scales all numerical columns using ScikitLearn StandardScaler and encodes all categorical columns Ordinally
- The Metrics used are a custom partial ROC-AUC metric (pAUC)- this evaluates the ROC-AUC metric only in the sections where False Negatives are low (malignant cases assigned to be benign)
- We also use the Recall Score and Confusion Matrix to evaluate our models
- Training and validation is done by Undersampling the initial data and using SMOTE to bring the distributions of 1s and 0s closer. This is done over K-Folds (K=5) to generalise model performance
- Cross Validation is done in a similar way but without using SMOTE. This is the metric that we report for the models
- Model Hyperparamaters were tuned using Optuna and the two final models considered are an Ensemble of XGBoost and LightGBM, and a PyTorch-TabNet architecture. While it was difficult to surely predict that one model works better than the other, the averaged K-Folds Inference seems to perform better for the Ensemble discussed. Thus we consider that our final Tabular Model
